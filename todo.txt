CURRENT TO DO:
1. Do some more tweaking of the solver till I'm happy enough with it
  - A little more speed testing of CPU vs GPU with the MLT solver. Also with different voltages.
  - Maybe try 2nd order optax things?
  - (LATER) Figure out how to use sparse matrices in jax
2. See if I can run 50 at once on a GPU
3. Huge DOE with all the parameters in a small network
4. Try RELU activation (random diodes)

IN PROGRESS:
- Work on multiprocessing with mp.Pool
  --> Next: How to pipe output to separate log files

RECENTLY DONE:
- Tweak the NL_mlt method so it considers KCL error. I'm decently happy with it now.
  - Make adaptive method that tries HYBR first, then TRF if there's KCL error, high ||res||, or I != i_in & i_out.
- Try shgo -- >5hr with no results. Granted, it was sharing resources on the GPU for part of that time.
- Try dual_annealing -- 1.5hrs to get a result.
- Try basinhopping -- Not working well
- Try differential_evolution -- SUUUUPER slow, with 500 nodes
- Compare CPU(np) w/ GPU w/ GPU-less jax. relative times: CPU 100, GPU 80, GPU-less 160
- 117 : Repeat 116 with edges instead of fibers. Hopefully we should be able to see which works better.
- 116_E : Repeat 116 with edges instead of fibers. This time with the same rands.
- 116_1r1164 - Compare MLT(HYBR) & GPU & TRF & MLT2 (corrected MLT)
- 116 : Try again, varying burn aggressiveness
- Try burning edges instead of fibers (rn.edge_burn)
- See which methods work will with a tiny RN, like N=16. Look for patterns.
- 115: Smaller network with longer fibers, trying to make XOR
- Fix it so that it will finish after Lcg if it's already within tolerance (e.g. v_in=0) -- done on branch jax
- Tried jax.jit version of spo.root, it turned out slower. Well, but that GPU was full at the time. Now it looks significantly faster.
114 : Looks like nothing exciting
113 : Vary the fiber length distribution stddev
- Note: None of the sims I've run since fixing the NL system are coming close. They're looking more linear; at least, I haven't had any 0b11 outputs lower than 0b01 or 0b10 outputs.
112_v3 : Repeat 112 with really NL system
- Run new FP on 100_18r2 --> Not XOR
- mlt method: xi from cg or hybr

OLDER DONE:
112_v2 : Repeat of 112, but with NL output current calculation.
112 : Repeat 97 (vary ks), with new burn technique.
111_v2 : Repeat of 111 but with new, adaptive burn_rate
111 : Same idea as 110, but with a smaller network
- fix how sum_currents uses Ohm's law
110 : Repeat 109 with slightly less sparsity & higher max_epochs
109 : Try to replicate 101
- Burn_rate # proportional to pin error. If it's nearly there, burn almost nothing.
  - Error metric: <90% --> 0 burning; 100% --> 50% burning; >110% --> 100% burning
- Report whether any of the pins are completely burnt out after each epoch. Save that in the csv so we can exclude those data points.
108 : Compare preburning to w/o preburning
107 : Try to replicate 100_18 - No successful.
106 : Vary ftype_proportions
105 : Try with lower ks
101B : Continuation of 101 (one difference: new power calculation)
102_v2 : Redo 102 with accurate NL power calculation.
102 : Redo 98 with // burn method.
101 : Redo 99 with new burn method.
100 : Fixed the burn so it's actually parallel! Redo 97.
99 : 8 copies of a larger RN with less burning per epoch.
- Add capacity to identify the nodes that were deleted between two RN states and plot those.
98 : Analyze varying fiber length. --> Inconclusive
- Fix the power calculation to be more accurate for nonlinear power. Earlier it was p=v**2/R, now it's p=v*i(G,w,v) (After sim 102)
- Fix burn method so it's actually //
- In 3d with fibers, since I can't change the fiber color easily in quiver, 
  add colored blobs underneath (scatter + zorder) for fiber_color
- 97 : vary ks from 1.6 to 1.9, with N=4.5k
- 95,96 : Vary ks from 1.5 to 1.9
- Add xtol to options
- Change the burn method so it burns anything with similar power levels to the highest power node (epsilon burn)
- Add argparser to main.py : Make it easier to run lots of different simulations by using one config file with different CLI options to supercede certain settings
- 89 - 92 : train_set sims with N=2k-2.5k. Now with 4-8 cycles.
- 93,94 : vary res_w and bpwr

Expansions:
- Scale / configure sim better to reality
  - https://www.conductivecomposites.com/pdf/Nanostrand_datasheet.pdf
  - Can I model Inductance & Capacitance? Will that be significant?
- Maybe add a probabalistic function for which burns
- Work on a version of NL_sol that has diodes on the outputs.
  - It doesn't seem that backwards currents happen naturally. I don't think it's worth the trouble since it would make the matrix non-symmetrical... (though that isn't a problem with some of the NL solvers)
- Separate the fwd_pass and RMSR calculation into two functions
- Things to try
  - Isolate the pins from each other with walls
    - Ooh, or we can burn between them! i0-i1 & o0-o1 (preburning)
  - Change order of training?
  - Nonlinear Feedback?

Questions:
- From physical research, do we know if removing edges or fibers is more realistic?

Speed Optimization:
- Time to find efficiency and complexity of each step & optimize
- Tune cg method by providing the guesses that make it faster and more accurate
- Time cg with different amounts of multithreading --> Fix it so it can do multithreading?
- Work on making n-k faster
- GPU acceleration
- (BACKBURNER) Optimize the performance of the solvers
  - rdiff and line_search for n-k
  - preconditioning? (Only for cg?)
  - smarter initial guesses?
  - It seems that hybr is faster, but n-k may have lower complexity. Maybe...
  - What if I always limit the number of iterations instead of having a set tolerance? That seems like a bad idea.

